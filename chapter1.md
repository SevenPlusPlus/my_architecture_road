---
## 微服务基础框架之路

### 微服务框架选型的背景

我们是在2014年中开始考虑这个事情，当时中国开源社区普遍使用rpc框架较多尤其是阿里开源的dubbo，springcloud当时受限于文档及推广等原因还没有那么被人所熟知，但dubbo开源项目那时也已经处于了一个不再维护的状态。

### 微服务框架选型我们关注的问题

- 考虑到公司正处于一个快速发展期，而且公司业务属于高频业务，所以性能是首先要考虑的问题。
- 作为微服务的核心框架，以及考虑到后续肯定会有定制维护的需求，所以必须自主可控。
- 考虑到我们当时团队规模不是很大，普遍技术能力也不是很强，所以最好是个轻量级的框架，但又应该有很好的可扩展性，以便后续团队大了之后的扩展开发。
- 考虑到当时大部分开发人员都是从单体应用开发转型过来，最好能贴近他们原先熟悉的开发编码模式，学习曲线小，便于框架的推广。

### 自研rpc框架kkrpc

我们选择自研rpc框架，但也不等于要全部重新造轮子，我们选择站在巨人的肩膀上（dubbo基础上）定制我们自己的rpc框架。我们主要做了以下几个方面的工作：

- 轻量化，dubbo在很多层面上都支持多种实现方案，如注册中心支持zk、redis等，网络层框架支持netty、grizzly等，而对我们来说只需要选定一种掌握即可，其他实现全部砍掉。
- dubbo中很多高级功能和配置对我们需求和应用场景来说都是不需要和复杂的，因此我们也都拿掉这些不必要的功能，只保留我们需要的核心功能并保证其稳定性。
- dubbo的网络层为了扩展性抽象封装了很多层，增加了维护的复杂度，我们也把这部分简化了，牺牲了一些对我们来说不必要的扩展性，但提高了简洁性和可维护性。
- 为了更好的性能，经过调研测试，我们序列化层改为使用protostuff、同时跨语言时支持hessian，同时为了较小传输包的overhead，我们重新设计了请求包结构。
- dubbo当时在服务治理方面其实是比较弱的，我们后来在参考了Netflix的hystrix的实现后，对其服务治理功能进行了增强，支持了服务降级、限制并发访问度、线程隔离、熔断等服务治疗手段，并研发了相应的管理后台供开发运维人员使用。
- 考虑到rpc框架在可测试性方面与http比的明显不足，我们研发了kkrpc专门的测试工具平台，用户只要上传其定义的api sdk jar包就可以选择特定服务实例填写方法参数进行测试，大大方便了研发测试人员。

### kkrpc 整体设计

![](/assets/RPC整体设计.png)

-	接口层：业务上定义服务接口并发布接口定义jar给服务消费者，服务提供者实现业务服务接口，对外提供自身业务服务；
-	配置层：对外配置接口，方便使用者配置服务消费或提供服务的各项参数。以ReferenceConfig为核心配置得到消费端配置RpcConfig，以此RpcConfig通过发布订阅层refer订阅得到消费端Invoker，再以此Invoker作为stub通过代理层getProxy得到接口代理实例；以ServiceConfig为核心配置得到提供端配置RpcConfig，通过代理层getInvoker对服务实现Target对象封装得到AbstractProxyInvoker，再将该Invoker作为服务端Skeleton通过发布订阅层export对外提供服务；
-	代理层：ProxyFactory对订阅得到的Invoker代理得到Proxy对象，将服务实现实例封装为Invoker注册发布对外提供服务；
-	发布订阅层：RegistryProtocol接受RpcConfig refer得到消费端stub Invoker，主要流程有：根据RpcConfig得到RegistryDirectory ，RegistryDirectory向zk registry注册消费临时节点，并订阅服务提供者目录节点 ，再由负载均衡Cluster实现 join RegistryDirectory得到CluserInvoker返回；将Skeleton Invoker根据RpcConfig配置调用Rpc协议层export对外提供服务，由RegistryDirectory向zk registry注册服务提供者临时节点；
-	负责均衡路由层：该层主要针对服务消费者作用。RegistryDirectory接收到zk registry发来的服务提供者节点变更通知List<RpcConfig>，经过route层路由，得到可达的服务提供者列表，经toInvokers通过Rpc协议层refer为stub Invoker列表供ClusterInvoker选择调用。ClusterInvoker从RegistryDirectory服务目录中list得到当前可用的提供者stub Invoker列表，经过loadbalancer最终select一个stub Invoker发起调用；
-	Rpc协议层：DefaultProtocol export接收Skeleton Invoker根据RpcConfig配置信息建立服务名到本地服务的映射，然后openServer创建NettyServer开始接受外部的RPC请求。DefaultProtocol refer接收RpcConfig新建到服务端的NettyClient连接集合，创建Stub DefaultInvoker对象（其中doInvoke方法则是通过NettyClient向服务对端发送RpcRequest请求）。DefaultProtocol扩展了ProtocolFilterWrapper和ProtocolListenerWrapper两个Wrapper实现分别用于Invoker调用过程中的FilterChain扩展和服务refer及export过程中的事件通知扩展；
-	网络传输层：NettyClient基于Netty的bootstrap实现跟服务对端建立连接、同步发送RpcRequest请求、同步非阻塞接收对端的RpcResponse结果包。其Channel pipeline添加了一系列ChannelHandler包括序列化反序列化Handler、业务处理Handler（HeartbeatChannelHandler【心跳包的处理】—>HeaderExchangeHandler【根据数据包类型实现数据交换-发送或接收】—>ExchangeHandler【真正处理业务请求】）。NettyServer基于Netty的ServerBootstrap实现服务端口的绑定接受并管理客户端连接、处理客户端发来的RpcRequest请求。其Channel pipeline添加了一系列ChannelHandler包括序列化反序列化Handler、业务处理Handler（HeartbeatChannelHandler【处理心跳包】—>ChannelHandlerDispatcher【将业务请求包派发给业务处理线程池并行处理】—>HeaderExchangeHandler【根据数据包类型实现数据交换-发送或接收】—> ExchangeHandler【真正处理业务请求】）；
-	序列化反序列化层：RpcPacketFrameDecoder根据协议包格式解码处理TCP粘包半包问题，RpcEncoder序列化包ChannelHandler，RpcDecoder反序列化包ChannelHandler，SerializationUtils序列化反序列化工具包封装实现了各种序列化反序列化实现。

### kkrpc 调用流程分析

![](/assets/RPC总体调用图.png)

### kkrpc 发布服务时序图

![](/assets/kkrpc-export-时序图.png)

### kkrpc 引用服务时序图

![](/assets/kkrpc-refer-时序图.png)

